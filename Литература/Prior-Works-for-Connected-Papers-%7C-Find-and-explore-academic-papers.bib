@article{71d1ac92ad36b62a04f32ed75a10ad3259a7218d,
title = {Anomaly detection: A survey},
year = {2009},
url = {https://www.semanticscholar.org/paper/71d1ac92ad36b62a04f32ed75a10ad3259a7218d},
abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
author = {V. Chandola and A. Banerjee and Vipin Kumar},
journal = {ACM Comput. Surv.},
volume = {41},
pages = {15:1-15:58},
doi = {10.1145/1541880.1541882},
}

@article{54e325aee6b2d476bbbb88615ac15e251c6e8214,
title = {Generative Adversarial Nets},
year = {2014},
url = {https://www.semanticscholar.org/paper/54e325aee6b2d476bbbb88615ac15e251c6e8214},
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to Â½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
author = {Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron C. Courville and Yoshua Bengio},
}

@article{a6cb366736791bcccc5c8639de5a8f9636bf87e8,
title = {Adam: A Method for Stochastic Optimization},
year = {2014},
url = {https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8},
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
author = {Diederik P. Kingma and Jimmy Ba},
journal = {CoRR},
volume = {abs/1412.6980},
pages = {null},
arxivid = {1412.6980},
}

@article{5d90f06bb70a0a3dced62413346235c02b1aa086,
title = {Learning Multiple Layers of Features from Tiny Images},
year = {2009},
url = {https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086},
abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
author = {A. Krizhevsky},
}

@article{5f5dc5b9a2ba710937e2c413b37b053cd673df02,
title = {Auto-Encoding Variational Bayes},
year = {2013},
url = {https://www.semanticscholar.org/paper/5f5dc5b9a2ba710937e2c413b37b053cd673df02},
abstract = {Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
author = {Diederik P. Kingma and M. Welling},
journal = {CoRR},
volume = {abs/1312.6114},
pages = {null},
arxivid = {1312.6114},
}

@article{abd1c342495432171beb7ca8fd9551ef13cbd0ff,
title = {ImageNet classification with deep convolutional neural networks},
year = {2012},
url = {https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
author = {A. Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
journal = {Communications of the ACM},
volume = {60},
pages = {84 - 90},
doi = {10.1145/3065386},
}

@article{00a1077d298f2917d764eb729ab1bc86af3bd241,
title = {Isolation Forest},
year = {2008},
url = {https://www.semanticscholar.org/paper/00a1077d298f2917d764eb729ab1bc86af3bd241},
abstract = {Most existing model-based approaches to anomaly detection construct a profile of normal instances, then identify instances that do not conform to the normal profile as anomalies. This paper proposes a fundamentally different model-based method that explicitly isolates anomalies instead of profiles normal points. To our best knowledge, the concept of isolation has not been explored in current literature. The use of isolation enables the proposed method, iForest, to exploit sub-sampling to an extent that is not feasible in existing methods, creating an algorithm which has a linear time complexity with a low constant and a low memory requirement. Our empirical evaluation shows that iForest performs favourably to ORCA, a near-linear time complexity distance-based method, LOF and random forests in terms of AUC and processing time, and especially in large data sets. iForest also works well in high dimensional problems which have a large number of irrelevant attributes, and in situations where training set does not contain any anomalies.},
author = {F. Liu and K. Ting and Zhi-Hua Zhou},
journal = {2008 Eighth IEEE International Conference on Data Mining},
volume = {null},
pages = {413-422},
doi = {10.1109/ICDM.2008.17},
}

@article{843959ffdccf31c6694d135fad07425924f785b1,
title = {Extracting and composing robust features with denoising autoencoders},
year = {2008},
url = {https://www.semanticscholar.org/paper/843959ffdccf31c6694d135fad07425924f785b1},
abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
author = {Pascal Vincent and H. Larochelle and Yoshua Bengio and Pierre-Antoine Manzagol},
doi = {10.1145/1390156.1390294},
}

@article{9acc51b06f54b07836fad4cc24633187dc21317f,
title = {A review of novelty detection},
year = {2014},
url = {https://www.semanticscholar.org/paper/9acc51b06f54b07836fad4cc24633187dc21317f},
abstract = {S2 TL;DR: This review aims to provide an updated and structured investigation of novelty detection research papers that have appeared in the machine learning literature during the last decade.},
author = {Marco A. F. Pimentel and D. Clifton and Lei A. Clifton and L. Tarassenko},
journal = {Signal Process.},
volume = {99},
pages = {215-249},
doi = {10.1016/j.sigpro.2013.12.026},
}

@article{5db790198b9acf4e5efe350acdd814238fcacaa7,
title = {Deep Anomaly Detection Using Geometric Transformations},
year = {2018},
url = {https://www.semanticscholar.org/paper/5db790198b9acf4e5efe350acdd814238fcacaa7},
abstract = {We consider the problem of anomaly detection in images, and present a new detection technique. Given a sample of images, all known to belong to a "normal" class (e.g., dogs), we show how to train a deep neural model that can detect out-of-distribution images (i.e., non-dog objects). The main idea behind our scheme is to train a multi-class model to discriminate between dozens of geometric transformations applied on all the given images. The auxiliary expertise learned by the model generates feature detectors that effectively identify, at test time, anomalous images based on the softmax activation statistics of the model when applied on transformed images. We present extensive experiments using the proposed detector, which indicate that our algorithm improves state-of-the-art methods by a wide margin.},
author = {I. Golan and Ran El-Yaniv},
journal = {ArXiv},
volume = {abs/1805.10917},
pages = {null},
arxivid = {1805.10917},
}
